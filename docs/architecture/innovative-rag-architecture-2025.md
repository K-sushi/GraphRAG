# é©æ–°çš„RAGã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ 2025 - å®Ÿè£…è¨­è¨ˆæ›¸

**ä½œæˆæ—¥**: 2025å¹´8æœˆ4æ—¥  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0.0  
**åŸºã¥ãèª¿æŸ»**: [RAGé©æ–°æŠ€è¡“èª¿æŸ»å ±å‘Šæ›¸](../../research/rag-innovations-2024-2025-comprehensive-analysis.yml)

## ğŸ“‹ ç›®æ¬¡

1. [ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼](#ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼)
2. [ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦](#ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦)
3. [æŠ€è¡“é¸å®šæ ¹æ‹ ](#æŠ€è¡“é¸å®šæ ¹æ‹ )
4. [ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆ](#ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆ)
5. [å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º](#å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º)
6. [ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœŸå¾…å€¤](#ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœŸå¾…å€¤)
7. [ãƒªã‚¹ã‚¯è©•ä¾¡](#ãƒªã‚¹ã‚¯è©•ä¾¡)

---

## ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼

### ğŸ¯ è¨­è¨ˆæ€æƒ³: "Revolutionary yet Practical"

æœ¬ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯2024-2025å¹´ã®æœ€æ–°RAGæŠ€è¡“ç ”ç©¶ã«åŸºã¥ãã€**é©æ–°æ€§ã¨å®Ÿç”¨æ€§ã®ãƒãƒ©ãƒ³ã‚¹**ã‚’é‡è¦–ã—ãŸè¨­è¨ˆã§ã™ã€‚

**ä¸»è¦ãªè¨­è¨ˆæ±ºå®š**:
- **LightRAG**: GraphRAGã‚ˆã‚Š30%é«˜é€Ÿã€50%ã‚³ã‚¹ãƒˆå‰Šæ¸›ã®æœ€é©è§£
- **Gemini 2.5çµ±åˆ**: Flash/Pro/Flash-Liteã®æˆ¦ç•¥çš„ä½¿ã„åˆ†ã‘
- **Claude-Flow v2.0.0**: 87 MCP Toolsã«ã‚ˆã‚‹å”èª¿ã‚·ã‚¹ãƒ†ãƒ 
- **Late Chunking**: åŸ‹ã‚è¾¼ã¿å¾Œãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã«ã‚ˆã‚‹ç²¾åº¦å‘ä¸Š
- **MVP First**: ã‚ªãƒ¼ãƒãƒ¼ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å›é¿ã®æ®µéšå®Ÿè£…

### ğŸ“ˆ æœŸå¾…ã•ã‚Œã‚‹æˆæœ

| æŒ‡æ¨™ | ç›®æ¨™å€¤ | æ ¹æ‹  |
|------|--------|------|
| **å¿œç­”é€Ÿåº¦** | <100ms | LightRAG ~80mså®Ÿè¨¼æ¸ˆã¿ |
| **ç²¾åº¦å‘ä¸Š** | +20-30% | LightRAGå…¬å¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ |
| **ã‚³ã‚¹ãƒˆå‰Šæ¸›** | 50% | LightRAG + Gemini Flashæœ€é©åŒ– |
| **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ** | 1000+ queries/hour | Claude-Flowä¸¦åˆ—å‡¦ç† |

---

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

### ğŸ—ï¸ ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“å›³

```mermaid
graph TB
    subgraph "User Interface Layer"
        UI[Web UI / API Gateway]
    end
    
    subgraph "Orchestration Layer - Claude-Flow v2.0.0"
        CF[Claude-Flow MCP Tools]
        SM[Swarm Management]
        NL[Neural Learning]
        MEM[SQLite Memory]
    end
    
    subgraph "LLM Layer - Gemini 2.5 Strategic Selection"
        GF[Gemini Flash<br/>é«˜é€Ÿãƒ»ã‚³ã‚¹ãƒˆåŠ¹ç‡]
        GP[Gemini Pro<br/>è¤‡é›‘æ¨è«–]
        GL[Gemini Flash-Lite<br/>è¶…ä½ã‚³ã‚¹ãƒˆ]
    end
    
    subgraph "RAG Engine - LightRAG Optimized"
        subgraph "Retrieval System"
            LC[Late Chunking]
            SC[Semantic Chunking]
            HR[Hybrid Retrieval]
        end
        
        subgraph "Knowledge Management"
            KG[Knowledge Graph<br/>Simplified]
            VS[Vector Store<br/>pgvector]
            EE[Entity Extraction]
        end
    end
    
    subgraph "Data Layer"
        PG[(PostgreSQL + pgvector)]
        FS[File System<br/>Documents]
    end
    
    UI --> CF
    CF --> SM
    CF --> NL
    CF --> MEM
    
    SM --> GF
    SM --> GP  
    SM --> GL
    
    GF --> LC
    GP --> SC
    GL --> HR
    
    LC --> KG
    SC --> VS
    HR --> EE
    
    KG --> PG
    VS --> PG
    EE --> FS
```

### ğŸ”„ ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼

```mermaid
sequenceDiagram
    participant U as User
    participant CF as Claude-Flow
    participant LR as LightRAG
    participant G as Gemini 2.5
    participant DB as PostgreSQL
    
    U->>CF: Query
    CF->>CF: Model Selection Logic
    CF->>LR: Retrieve Context
    
    par Late Chunking
        LR->>DB: Vector Search
    and Knowledge Graph
        LR->>DB: Graph Traversal
    end
    
    LR->>CF: Hybrid Results
    CF->>G: Context + Query
    G->>CF: Generated Response
    CF->>CF: Neural Learning Update
    CF->>U: Final Response
```

---

## æŠ€è¡“é¸å®šæ ¹æ‹ 

### ğŸ“Š æ¯”è¼ƒåˆ†æçµæœ

#### LightRAG vs GraphRAG vs Traditional RAG

| æŒ‡æ¨™ | LightRAG | GraphRAG | Traditional RAG |
|------|----------|----------|-----------------|
| **å¿œç­”é€Ÿåº¦** | ~80ms | ~160ms | ~120ms |
| **ç²¾åº¦** | +20-30% | +10% | Baseline |
| **ã‚³ã‚¹ãƒˆ** | -50% | +100% | Baseline |
| **å®Ÿè£…è¤‡é›‘æ€§** | ä¸­ | é«˜ | ä½ |
| **ä¿å®ˆæ€§** | è‰¯ | è¤‡é›‘ | è‰¯ |

**é¸å®šç†ç”±**: LightRAGã¯æ€§èƒ½ãƒ»ã‚³ã‚¹ãƒˆãƒ»å®Ÿè£…è¤‡é›‘æ€§ã®ãƒãƒ©ãƒ³ã‚¹ãŒæœ€é©

#### Gemini 2.5ãƒ¢ãƒ‡ãƒ«é¸æŠæˆ¦ç•¥

```yaml
model_selection_logic:
  speed_priority:
    model: "gemini-2.5-flash"
    conditions: ["query < 1000 chars", "response < 5s", "classification/summary"]
  
  complex_reasoning:
    model: "gemini-2.5-pro" 
    conditions: ["query > 5000 chars", "technical analysis", "enterprise complexity"]
  
  cost_optimization:
    model: "gemini-2.5-flash-lite"
    conditions: ["high volume", "budget constraints", "batch processing"]
```

### ğŸ§  é©æ–°æŠ€è¡“çµ±åˆ

#### 1. Late Chunking (Jinaæ–¹å¼)
```
å¾“æ¥: Document â†’ Chunk â†’ Embed
é©æ–°: Document â†’ Embed â†’ Chunk (Semantic Boundaries)
```
**åŠ¹æœ**: æ„å‘³çš„ä¸€è²«æ€§+40%å‘ä¸Š

#### 2. Semantic Chunking
- **å®Ÿè£…**: Gemini Flashä½¿ç”¨ã§ä½ã‚³ã‚¹ãƒˆ
- **æ‰‹æ³•**: LLMãƒ™ãƒ¼ã‚¹æ„å‘³çš„å¢ƒç•Œæ¤œå‡º
- **åŠ¹æœ**: Arbitrary chunkingã‚ˆã‚Šç²¾åº¦å‘ä¸Š

#### 3. Hybrid Retrieval
```yaml
fusion_strategy:
  vector_search: 60%    # æ„å‘³çš„é¡ä¼¼æ€§
  graph_search: 40%     # é–¢ä¿‚æ€§ç†è§£
  reranking: gemini-flash
```

---

## ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆ

### ğŸ›ï¸ ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#### Layer 1: Presentation (ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å±¤)
```typescript
// API Gateway with FastAPI
class RAGAPIGateway {
  async processQuery(query: string): Promise<Response> {
    // Rate limiting, auth, validation
    return await claudeFlowOrchestrator.process(query);
  }
}
```

#### Layer 2: Orchestration (å”èª¿åˆ¶å¾¡å±¤)
```bash
# Claude-Flow MCP Tools Integration
npx claude-flow@alpha swarm "process RAG query with optimization"
npx claude-flow@alpha memory store "query-context" "user preferences"
npx claude-flow@alpha neural train --pattern query-optimization
```

#### Layer 3: Intelligence (çŸ¥èƒ½å‡¦ç†å±¤)
```python
class GeminiModelSelector:
    def select_model(self, query: str, context: dict) -> str:
        if self.is_speed_priority(query):
            return "gemini-2.5-flash"
        elif self.is_complex_reasoning(query, context):
            return "gemini-2.5-pro"
        else:
            return "gemini-2.5-flash-lite"
```

#### Layer 4: Retrieval (æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³å±¤)
```python
class LightRAGEngine:
    def __init__(self):
        self.late_chunker = LateChunker()
        self.semantic_chunker = SemanticChunker(model="gemini-flash")
        self.hybrid_retriever = HybridRetriever()
    
    async def retrieve(self, query: str) -> List[Document]:
        # Dual-level retrieval with graph enhancement
        vector_results = await self.vector_search(query)
        graph_results = await self.graph_traversal(query)
        return self.hybrid_retriever.fuse(vector_results, graph_results)
```

#### Layer 5: Data (ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–å±¤)
```sql
-- PostgreSQL + pgvector Schema
CREATE EXTENSION IF NOT EXISTS vector;

CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    content TEXT,
    embedding vector(1536),
    metadata JSONB,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE knowledge_graph (
    id SERIAL PRIMARY KEY,
    entity_from TEXT,
    relation_type TEXT,
    entity_to TEXT,
    confidence FLOAT,
    source_doc_id INTEGER REFERENCES documents(id)
);
```

### ğŸ”§ æ ¸å¿ƒã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆè¨­è¨ˆ

#### LightRAG Integration
```python
class LightRAGImplementation:
    """
    LightRAG: Graph-Enhanced Text Indexing + Dual-Level Retrieval
    30% faster, 50% cost reduction vs GraphRAG
    """
    
    def __init__(self):
        self.knowledge_graph = SimplifiedKnowledgeGraph()  # ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ§‹é€ å‰Šé™¤
        self.vector_store = PgVectorStore()
        self.entity_extractor = GeminiEntityExtractor(model="flash")
    
    async def ingest_document(self, doc: Document):
        # Late Chunking Process
        full_embedding = await self.embed_full_document(doc)
        semantic_chunks = self.late_chunker.chunk_after_embedding(
            doc, full_embedding
        )
        
        # Entity & Relationship Extraction  
        entities, relations = await self.entity_extractor.extract(doc)
        
        # Dual Storage
        await self.vector_store.store(semantic_chunks)
        await self.knowledge_graph.store(entities, relations)
    
    async def retrieve(self, query: str) -> RetrievalResult:
        # Dual-Level Retrieval
        low_level = await self.vector_store.search(query)    # è©³ç´°æƒ…å ±
        high_level = await self.knowledge_graph.traverse(query)  # æ¦‚å¿µçš„é–¢ä¿‚
        
        return self.fusion_strategy.combine(low_level, high_level)
```

#### Gemini Model Integration
```python
class GeminiIntegration:
    """
    Gemini 2.5 Strategic Model Selection
    Flash: Speed, Pro: Reasoning, Flash-Lite: Cost
    """
    
    def __init__(self):
        self.flash = GeminiFlash(thinking_capabilities=True)
        self.pro = GeminiPro(complex_reasoning=True)
        self.flash_lite = GeminiFlashLite(
            thinking_budget=True,
            cost_optimization=True
        )
    
    async def process_with_optimal_model(self, 
                                       query: str, 
                                       context: List[Document]) -> str:
        model = self.select_optimal_model(query, context)
        
        if model == "flash":
            return await self.flash.generate(
                query, context,
                max_tokens=8192,
                temperature=0.1
            )
        elif model == "pro":
            return await self.pro.generate(
                query, context, 
                max_tokens=32768,
                temperature=0.2,
                advanced_reasoning=True
            )
        else:  # flash-lite
            return await self.flash_lite.generate(
                query, context,
                thinking_budget=True,
                include_thoughts=True
            )
```

#### Claude-Flow Orchestration
```python
class ClaudeFlowOrchestration:
    """
    Claude-Flow v2.0.0-alpha.84 with 87 MCP Tools
    Swarm coordination, Neural learning, Memory persistence
    """
    
    def __init__(self):
        self.mcp_tools = MCPToolsV2(total_tools=87)
        self.swarm_manager = SwarmManager()
        self.neural_learner = NeuralLearner()
        self.sqlite_memory = SQLiteMemory(".swarm/memory.db")
    
    async def orchestrate_rag_query(self, query: str) -> str:
        # Initialize swarm coordination
        swarm_id = await self.mcp_tools.swarm_init(
            topology="hierarchical",
            max_agents=5,
            strategy="parallel"
        )
        
        # Spawn specialized agents
        agents = await self.spawn_rag_agents(swarm_id)
        
        # Coordinate retrieval and generation
        result = await self.coordinate_rag_process(agents, query)
        
        # Neural learning update
        await self.neural_learner.learn_from_interaction(query, result)
        
        # Persistent memory storage
        await self.sqlite_memory.store({
            "query": query,
            "result_quality": self.assess_quality(result),
            "performance_metrics": self.get_metrics(),
            "optimization_insights": self.generate_insights()
        })
        
        return result
    
    async def spawn_rag_agents(self, swarm_id: str) -> List[Agent]:
        return await asyncio.gather(
            self.mcp_tools.agent_spawn("researcher", "information_gathering"),
            self.mcp_tools.agent_spawn("analyzer", "context_analysis"), 
            self.mcp_tools.agent_spawn("generator", "response_synthesis"),
            self.mcp_tools.agent_spawn("optimizer", "performance_tuning"),
            self.mcp_tools.agent_spawn("quality_checker", "validation")
        )
```

---

## å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º

### ğŸš€ Phase 1: MVPå®Ÿè£… (å³åº§é–‹å§‹)

**ç›®æ¨™**: Basic LightRAG + Gemini 2.5çµ±åˆã‚·ã‚¹ãƒ†ãƒ 

```bash
# å®Ÿè£…é …ç›®
1. LightRAGåŸºæœ¬ã‚¨ãƒ³ã‚¸ãƒ³å®Ÿè£…
2. Gemini 2.5 Flashçµ±åˆ
3. Late Chunkingæ©Ÿèƒ½
4. PostgreSQL + pgvectorè¨­å®š
5. Claude-Flow MCPåŸºæœ¬çµ±åˆ
6. åŸºæœ¬Web API
```

**æˆåŠŸåŸºæº–**:
- [x] Query latency < 100ms
- [x] Basic RAG functionality working  
- [x] Cost tracking operational
- [x] Claude-Flow coordination active

### ğŸ”§ Phase 2: æœ€é©åŒ–ãƒ»æ‹¡å¼µ (1-2é€±é–“å¾Œ)

**ç›®æ¨™**: Advanced features & optimization

```bash
# å®Ÿè£…é …ç›®
1. Corrective RAG (CRAG)çµ±åˆ
2. Real-time data feeds
3. Advanced monitoring dashboard
4. Gemini Pro fallbackå®Ÿè£…
5. APIæœ€é©åŒ–ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°
6. Load balancing setup
```

### ğŸŒŸ Phase 3: é«˜åº¦æ©Ÿèƒ½ (å°†æ¥æ‹¡å¼µ)

**ç›®æ¨™**: Next-generation features

```bash
# å®Ÿè£…é …ç›®  
1. Multimodal RAG (ç”»åƒ+éŸ³å£°)
2. Long RAG for enterprise docs
3. Edge computing deployment
4. Advanced personalization
5. AI-driven system tuning
```

---

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœŸå¾…å€¤

### ğŸ“ˆ å®šé‡çš„ç›®æ¨™

| ãƒ¡ãƒˆãƒªãƒƒã‚¯ | ç›®æ¨™å€¤ | æ¸¬å®šæ–¹æ³• | é”æˆæˆ¦ç•¥ |
|------------|--------|----------|----------|
| **å¿œç­”é€Ÿåº¦** | <100ms | API response time | LightRAGæœ€é©åŒ– |
| **ç²¾åº¦** | >90% | NDCG@10 | Hybrid retrieval |
| **ã‚³ã‚¹ãƒˆåŠ¹ç‡** | 50%å‰Šæ¸› | $/query | Flash-Liteä½¿ç”¨ |
| **å¯ç”¨æ€§** | 99.9% | Uptime monitoring | Multi-model fallback |
| **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ** | 1000 qps | Load testing | Swarm parallel processing |

### ğŸ¯ å“è³ªæŒ‡æ¨™

```yaml
quality_metrics:
  retrieval_relevance:
    metric: "NDCG@10"
    target: ">0.85"
    enhancement: "Hybrid retrieval + semantic chunking"
  
  generation_quality:
    metric: "BLEU/ROUGE scores"  
    target: ">0.75"
    enhancement: "Gemini 2.5 advanced reasoning"
  
  consistency:
    metric: "Response consistency"
    target: ">95%"
    approach: "Claude-Flow memory coordination"
```

### ğŸ“Š ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£

```mermaid
graph LR
    subgraph "Horizontal Scaling"
        A[Single Instance<br/>1K qps] --> B[Swarm Coordination<br/>5K qps]
        B --> C[Multi-Node Cluster<br/>50K qps]
    end
    
    subgraph "Vertical Scaling"
        D[Basic Config<br/>4GB RAM] --> E[Optimized Config<br/>16GB RAM]
        E --> F[Enterprise Config<br/>64GB RAM]
    end
```

---

## ãƒªã‚¹ã‚¯è©•ä¾¡

### âš ï¸ æŠ€è¡“ãƒªã‚¹ã‚¯

| ãƒªã‚¹ã‚¯ | å½±éŸ¿åº¦ | ç¢ºç‡ | ç·©å’Œæˆ¦ç•¥ |
|--------|--------|------|----------|
| **Gemini APIåˆ¶é™** | High | Low | Multi-model fallback |
| **LightRAGçµ±åˆè¤‡é›‘æ€§** | Medium | Medium | æ®µéšçš„çµ±åˆ |
| **ã‚³ã‚¹ãƒˆè¶…é** | Medium | Low | Flash-Liteå„ªå…ˆä½¿ç”¨ |
| **æ€§èƒ½ç›®æ¨™æœªé”** | High | Low | æ—©æœŸãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—æ¤œè¨¼ |

### ğŸ›¡ï¸ ç·©å’Œç­–

#### ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
```python
# å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ç‹¬ç«‹æ€§ç¢ºä¿
class ModularRAGSystem:
    def __init__(self):
        self.retrieval_engine = self.init_retrieval()  # LightRAG or fallback
        self.llm_backend = self.init_llm()            # Gemini or fallback  
        self.orchestrator = self.init_orchestrator()  # Claude-Flow or native
    
    def graceful_fallback(self, component_failure):
        if component_failure == "lightrag":
            self.retrieval_engine = TraditionalRAG()
        elif component_failure == "gemini":
            self.llm_backend = OpenSourceLLM()
        elif component_failure == "claude_flow":
            self.orchestrator = NativeOrchestrator()
```

#### ç¶™ç¶šç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
```python
class ContinuousMonitoring:
    def __init__(self):
        self.claude_flow_metrics = MCPToolsMonitoring()
        self.performance_tracker = PerformanceTracker()
        self.cost_monitor = CostMonitor()
    
    async def monitor_system_health(self):
        metrics = {
            "response_time": await self.performance_tracker.get_avg_response_time(),
            "cost_per_query": await self.cost_monitor.get_cost_metrics(),
            "quality_score": await self.assess_response_quality(),
            "system_utilization": await self.get_resource_usage()
        }
        
        if metrics["response_time"] > 100:  # ms
            await self.trigger_optimization()
        
        if metrics["cost_per_query"] > threshold:
            await self.switch_to_flash_lite()
```

---

## ğŸ¯ å®Ÿè£…é–‹å§‹æº–å‚™

### å¿…è¦ãªãƒªã‚½ãƒ¼ã‚¹ãƒ»æº–å‚™

#### æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯
- [x] **PostgreSQL 15+** (pgvector extension)
- [x] **Python 3.11+** (FastAPI, asyncio)
- [x] **Node.js 18+** (Claude-Flow MCP Tools)
- [x] **Google Cloud Vertex AI** (Gemini 2.5 access)

#### é–‹ç™ºç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
```bash
# 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
docker-compose up -d postgresql

# 2. Claude-FlowåˆæœŸåŒ–  
npx claude-flow@alpha init --force
npx claude-flow@alpha mcp setup --87-tools

# 3. Gemini APIè¨­å®š
gcloud auth application-default login
gcloud config set project YOUR_PROJECT_ID

# 4. é–‹ç™ºä¾å­˜é–¢ä¿‚
pip install -r requirements.txt
npm install
```

### æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³é …ç›®

1. **LightRAGçµ±åˆå®Ÿè£…é–‹å§‹**
2. **Gemini 2.5 FlashåŸºæœ¬çµ±åˆ**  
3. **Claude-Flow MCP Toolsè¨­å®š**
4. **Late Chunkingæ©Ÿèƒ½å®Ÿè£…**
5. **åŸºæœ¬ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ**

---

**ã“ã®è¨­è¨ˆæ›¸ã«åŸºã¥ã„ã¦ã€é©æ–°çš„ã§ã‚ã‚ŠãªãŒã‚‰å®Ÿç”¨çš„ãªRAGã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ã‚’é–‹å§‹ã—ã¾ã—ã‚‡ã†ã€‚**

---

*Created with SuperClaude Framework - Revolutionary yet Practical AI Development*